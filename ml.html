<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Artificial Intelligence</title>
    <style>
        *{box-sizing: border-box;margin: 0;padding: 0;}
        body{
            margin: 0;
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
        }
        .menu{
                background-color: black;
                display: flex;
                flex-wrap: wrap;
                justify-content: space-evenly;
                color: white;
            }
            .menu a{
                padding: 10px 30px;
                color: white;
                text-decoration: none;
                transition: .5s;
            }
            .menu a:hover{
                color: black;
                background-color: white;
            }
        .header{
            width: 100%;
            padding: 20px;
            padding-left: 20px;
            padding-right: 20px;
            border-bottom: 1px solid #ddd;
            display:flex;
            justify-content: space-evenly;
            align-items: center;
            background: #1779ff;
        }
        .header a{
            text-decoration: none;
            color: white;
        }
        .header a:hover{
            color: black;
        }
        .section{
            flex: 100%;
            padding: 20px;
            background-color: lightgrey;
            margin-bottom: 20px;
            position: relative;
       }
       .section b{
           font-size: 40px;
       }
       .section p{
            font-size: 25px;
       }
        .footer{
            width: 100%;
            padding: 20px;
            padding-left: 20px;
            padding-right: 20px;
            border-bottom: 1px solid #ddd;
            display:flex;
            justify-content: space-evenly;
            align-items: center;
            background: #333;
            color: white;
        }
        .imgdiv{
            width: 100%;
            height: 100%;
            object-fit: cover;
            background-repeat: no-repeat;
            border-radius: 6px;
            padding: 20px;
        }
        .section1 p{
            color: black;
            font-size: 20px;
        }
        .section1 b{
            font-size: 20px;
        }
    </style>
</head>
<body>
    <div class="menu">
        <a href="home.html">HOME</a>
        <a href="aboutus.html">ABOUT</a>
        <a href="service.html">FEEDBACK</a>
        <a href="contact1.html">SIGN IN</a>
    </div>
    <div class="header">
        <span>
            <a href="ai.html">ARTIFICIAL INTELLIGINCE</a>
        </span>
        <span> 
        <a href="index.html">MACHINE LEARNING</a>
        </span>
        <span> 
            <a href="network.html">NEURAL NETWORKS</a>
        </span>
    </div>
    <div class="section">
        <b>Abstract Intelligence</b>
        <br><br>
        <p>Symbolic reality is the basis for intelligence, in general.</p>
            <div class="box"> 
                <div class="imgdiv">
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                    <img src="ml.jpeg" alt="" height="350px" width="550px">
                </div>            
            </div>
        <div class="section1">
            <p>Technology, and reality, is dependent on a symbol. All the way back to ancient yin and yang, to modern zero and one. Therefore, it helps to understand the symbol.</p>
            <br>
            <p>If I tell you zero and one are X and Y. You’ll get it. If I tell you zero and one are circumference and diameter, half-of-you will get it. Collective. And, individual.</p>
            <br>
            <div class="box"> 
                <div class="imgdiv">
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                    <img src="ml1.jpeg" alt="" height="200px" width="150px">
                </div>            
            </div>
            <p>Zero and one. Circumference and diameter.</p>
            <br>
            <p>There is a circular relationship between an individual and a group (self-sufficient state). All systems and disciplines. Explaining the symbolic relationship between man and nature. Food, clothing, shelter. Communication, transportation, respiration.</p>
            <br>
            <p>Therefore, the rest is easy.</p>
            <br>
        </div>    
        <div class="section">
            <b>Training AI with CGI</b><br><br>
            <p>We used only synthetic data to train a computer vision model to identify components on a raspberry pi board.</p>
            <div class="box"> 
                <div class="imgdiv">
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                    <img src="mi1.png" alt="" height="350px" width="550px">
                </div>            
            </div>
            <div class="section1">
                <p>In this article we go through how we trained a computer vision model (AI) to detect sub-components of a Raspberry PI using only synthetic data (CGI).</p>
                <br>
                <p>Training with synthetic data is an increasingly popular way to quench the thirst of data hungry deep learning models. The datasets used for this project are available for free at app.zumolabs.ai [1]. We want to make using synthetic data easy for everyone and plan to release more datasets in the future.</p>
                <br>
                <b>The Problem</b>
                <br>
                <p>The Raspberry Pi is a single-board computer very popular with hobbyists. Our goal was to detect some of the sub-components that sit on the board: the pin connectors, the audio jack, and the ethernet port. Though this is a toy problem, it is not far from what you see in the real world — where automating component and defect detection using computer vision can improve the speed and reliability of manufacturing.</p>
                <br>
                <b>The Data</b>
                <br>
                <p>To generate synthetic data, we first need a 3D model of the object. Luckily, in today’s world, most objects already exist in the virtual world. Asset aggregation sites like SketchFab, TurboSquid, or Thangs, have commoditized 3D models [2]. Tip for the wise: if you can’t find a model on the internet, try contacting the manufacturer directly, or scan and model the object yourself.</p>
                <br>
                <p>We then use a game engine (such as Unity or Unreal Engine) to take thousands of images of our 3D model from a variety of camera angles and under a variety of lighting conditions. Each image has a corresponding segmentation mask, which sections out the different components in the image. In future articles we will dive deeper into the process of creating synthetic images (so stay tuned!).</p>
                <br>
                <p>So now that we have thousands of synthetic images, we should be good right? No! It is very important to test synthetically-trained models on real data to know whether the model is successfully generalizing to real data. There is a gap between simulation-produced data and real data known as the sim2real gap. One way to think about it is that deep learning models will overfit on the smallest of details, and if we aren’t careful many of these details may only exist in the synthetic data.</p>
                <br>
                <p>For this problem, we manually annotated a small test dataset of a dozen real images. Manual annotation is time consuming and expensive. Important to note that if we were using real images for training we would have to manually annotate thousands of images instead of just a handful for testing! That’s, unfortunately, the current way of doing things, the status quo we are trying to change. Getting rid of this manual annotation process is a critical step in building better AI.</p>
                <br>
                <p>One way we can start closing the sim2real gap is through a technique known as domain randomization [3][4]. This strategy involves randomizing properties of the virtual RPi, especially the visual appearance of the backgrounds and the RPi itself. This has the downstream effect of making the model we train on this data more robust to variations in color and lighting. This is also known as the network’s ability to generalize.</p>
                <br>
                <div class="box"> 
                    <div class="imgdiv">
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                        <img src="ml2.png" alt="" height="350px" width="550px">
                    </div>            
                </div>
                <br>
                <p>Domain randomized images.</p>
                <br>
                <div class="box"> 
                    <div class="imgdiv">                        
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                        <img src="ml4.png" alt="" height="350px" width="600px">
                    </div>            
                </div>
                <br>
                <p>Domain randomization: increasing the variance of the synthetic data distribution.</p>
                <br>
                <p>The Model and Training</p>
                <br>
                <p>Now we get to the model. There are many different types of computer vision models to choose from. Models which leverage deep learning are the most popular right now. They work very well for detection tasks, such as this project. We used a model from PyTorch’s torchvision library based on the ResNet architecture [5]. Synthetic data will work with any model architecture, so feel free to experiment and find the one that best fits your use case.</p>
                <br>
                <p>We trained our model with four different synthetic datasets to show how domain randomization and dataset size affect performance on our real test dataset:</p>
                <br>
                <p>Dataset A — 15 thousand realistic synthetic images.</p>
                <br>
                <p>Dataset B — 15 thousand domain randomized synthetic images.</p>
                <br>
                <p>Dataset C — 6 thousand realistic synthetic images.</p>
                <br>
                <p>Dataset D — 6 thousand domain randomized synthetic images.</p>
                <br>
                <div class="box"> 
                    <div class="imgdiv">
                        
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                        <img src="ml3.png" alt="" height="350px" width="600px">
                    </div>            
                </div>
                <br>
                <p>Mean Average Precision on synthetic data compared to real data.</p>
                <br>
                <p>We use mAP (mean average precision) to measure the performance of our computer vision model. It is important to note that performance metrics can be very arbitrary, so make sure to look at model predictions to make sure your model is performing as it should. As we predicted, the performance of the models increases with the more synthetic data we use. Deep learning models will almost always improve with larger datasets, but, more interestingly, training with a domain randomized synthetic dataset results in a significant performance boost over our real test dataset.</p>
                <br>
                <b>Conclusion</b>
                <br>
                <p>TLDR: in this article, we trained a computer vision model to detect sub-components of a Raspberry PI using entirely synthetic data. We used the technique of domain randomization to improve the performance of our model on real images. And, ta-da! Our trained model works on real data despite it never having seen a single real image.</p>
                <br>
            </div>
        </div>
    </div>
    <div class="footer">
        &copy;All Rights Reserved. Developed by Aashi Shah 
    </div>
</body>
</html>